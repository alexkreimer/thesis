\chapter{Monocular Scale}

\section{Introduction}

Recovering camera 6-DOF ego-motion from images is a well studied
problem. It arises in various practical contexts
(e.g. virtual/augmented reality application, autonomous or aided
navigation, etc.).  The problem was studied in both stereo and the
monocular setups.  To recover the full 6-DOF motion, the previous
works resorted to the stereo setup, used auxiliary sensors (e.g. IMU)
or rely on the planar motion assumption.  All of these have their
drawbacks: stereo pairs are fragile and require careful calibration
procedures, additional sensors are not always available and also
require calibration, scene assumptions don't always hold.  Motion
estimation from images of a single moving camera is probably the
hardest setup, as well as the most desirable one, because of its
simplicity.  It is well known that the translation scale parameter is
not directly observable for a motion of a single camera.

We argue, that for natural scenes the scale information is present in
the images (see e.g.~\ref{fig:feature_vectors}).  We propose to apply
statistical machine learning techniques to learn the monocular scale.
In the following chapters we present an overview of the existing works
in the field, describe our method and present the experimental
results.

\section{Related work}
\paragraph{Geometry based methods} One category of works proceed by
making assumptions on the camera motion. In~\cite{gutierrez2012full}
the authors assume head-mounted camera and relate the vertical camera
oscilation frequence to the step length and, subsequently, to person
height, which allows full 6DOF motion
estimation. ~\cite{scaramuzza2009absolute} assumes that the vehicle
adheres to the Ackerman steering principle and exploit the
non-holomicity of its motion, to compute the scale.  Another family of
works make planar vehicle motion assumption and assume that the height
of the camera is known.  These works assume that the images are taken
by a camera, rigidly mounted on the car that drives on a flat
road. Usually, they contain a number of similar steps: begin by
defining a region of interest (the planar road surface), proceed by
matching features in this ROI and estimating the ground plane
homography that relates subsequent images and then refine the results
by means of non-linear optimization step.  ~\cite{zhou2016reliable}
contribute a divide-and-conquer approach, where they decompose the
ground plane homography into the structure and the motion parameters.
Their optimization step is based on this division.
~\cite{kitt2011monocular} take a more SLAM-like approach by collecting
and keeping track of a constand number of ground-plane features which
they use to refine the scale.  ~\cite{grater2015robust} complement the
ground plane estimation based on the structure from motion techniques
with vanishing point estimation which renders the algorithm robust in
urban scenarios. ~\cite{7995955} considers both dense and sparse cues
for ground plane estimation, this makes their method robust toward
varying availability and distribution of trackable ground structure.
They parameterize and optimize over the motion parameters directly,
instead of optimizing the ground plane homography.  Furthermore they
use classifiers based on e.g., moments and residuals to reject scale
outliers.  This work seems to achieve best known results of monocular
scale estimation.  We compare against their results.


\paragraph{Learning methods} We are not aware of works that directly
learn the monocular scale. Yet there are a number of works that use
machine learning approaches to solve the 6DOF visual odometry.
Optical flow is used to train k-nearest neighbour, gaussian processes
and support vector machines in ~\cite{roberts2008memory},
~\cite{guizilini2013semi}, ~\cite{CIARFUGLIA20141717}
respectively. ~\cite{wang2017deepvo} shows that visual odometry may be
addressed in the end-to-end fashion, using the deep learning
techniques, the authors use CNN to learn the geometric feature
representation and the recurent neural networks to model sequential
motion dynamics. In~\cite{muller2017flowdometry} the authors use
optical flow images as CNN input to learn and infer camera
motion. Authors in~\cite{DBLP:journals/corr/MohantyADGSC16} also
propose to train a CNN.



\subsection{Our method}

We assume that a single camera moves through space and takes images.
We treat the initial camera pose (at time $t=0$) as the world
coordinate frame.  We denote the pose of the camera at time $t$ by
$\mathbf{\hat{T}}_t$ described by the rotation matrix
$\mathbf{\hat{R}}_t$ and the translation vector $\mathbf{\hat{t}}_t$
as seen in the world coordinate frame.  We denote camera image taken
at time $t$ by $I_t$.  To facilitate the discussion, we also introduce
notation for camera pose
$\mathbf{T}_t = [\mathbf{R}_t\ |\ \mathbf{t}_t] $ as seen from the
coordinate frame associated with camera pose at time $t-1$.  Most of
the time, we will omit the time index, since its clear from the
context.  By translation scale (or simply, scale) we refer to the norm
of the translation vector $\mathbf{t}$ (e.g.,
$s = \lVert \mathbf{t} \rVert$)

We pose the scale estimation problem as a regression problem and
search for a good regressor model.

\section{Random forest}

\subsection{Decision trees}

In this section we briefly describe tree-based methods for regression.
These involve splitting the feature space into a number of small
regions.  The prediction for a sample is made by computing a mean or a
mode of training samples that belong to the same region.  Since the
set of rules used to split the feature space into smaller regions may
be described by a tree, these methods are referred to as
\textit{decision tree} methods.

Building a decision tree may be described by a two step procedure:
\begin{enumerate}
\item Split the feature space, e.g., the set of all possible values
  for $X_1, X_2,\ldots,X_n$ into $J$ distinct regions $R_1, R_2,\ldots, R_J$.
\item For every sample that belongs to the regions $R_i$ we make the
  same prediction, which is a mean of the responses of training
  samples that belong to this region.
\end{enumerate}

The regions $R_i$ are usually chosen to be multidimensional boxes (axis aligned).  We would like to find such a partition of the feature space that minimizes
\begin{equation}\label{eq:tree_objective}
\sum\limits_{j=1}^J\sum_{x\in R_j}{(x-\mean{y}_j)^2}
\end{equation}

Where $y_j$ denotes the mean of the response values of the
samples in region $R_j$.  Unfortunately, solving the optimization
problem~\ref{eq:tree_objective} is computationally hard.  Usually it
is replaced with a greedy algorithm, called \textit{recursive binary
  splitting}.  This approach starts at the top of the tree and
greedily chooses the best split at that point that minimizes the
variance of its sub-trees.  To be more precise, for each $j$ and $s$ we
define the hyper-planes:

\begin{equation}
  R_1(j,s) = \{ X\lvert X_j<s \}\quad\text{and}\quad R_2(j,s) = \{ X\lvert X_j \geq s \},
\end{equation}

We seek such $j$ and $s$ that minimize the equation:

\begin{equation}
  \sum\limits_{i:x_i\in R_1(j,s)}{(x_i-y_{R_1})}^2 + \sum\limits_{i:x_i\in R_2(j,s)}{(x_i-y_{R_2})}^2,
\end{equation}

Where $y_{R_1}, y_{R_2}$ are the average responses of the samples in
$R_1(j,s), R_2(j,s)$ respectively.  Once we found the $j$ and $s$ we
recursively split each sub-tree in a similar manner.  The process is
repeated until a stopping criterion (e.g., number of nodes in the
leaf) is reached.

\subsection{Bagging}
Decision trees tend to suffer from \textit{high variance}. This means
that if we split the training set into a number of random subsets and
fit random tree into each sub-sample, we would likely to get much
different answers from these trees when asked the same question.  It
is known that the variance of a mean of a set of independent random
variables is $\frac{1}{n}$.  Thus, in order to improve the variance of
the estimator, it is possible to fit $n$ estimators, each to its
training-set and the average their predictions.  Since, usually, we
don't have $n$ training sets, we would use \textit{bootstrapping}
(e.g., sample independently with replacement from the data set).

\subsection{Random Forest}
The random forest suggests an additional improvement over bagging, by
decorrelating the random trees.  The issue they attempt to address is
this: lets say there is a very dominant feature w.r.t. to task at hand
for a given data-set.  Bagging ensures that we use different training
sets, but yet, most trees will tend to first split on this dominant
feature.  In this case the trees will resemble each other.  In random
forest, the trees are constructed by using only a subset of features
(e.g., $m = \sqrt{p}$).  This means that at calculating the splits,
the algorithm is allowed to consider only a subset of features.

\section{Convolutional Neural Networks}

Convolutional neural networks (CNN) leverage the availability of the
computational power and the abundance of data.  CNN have become a
method of choice in a number of computer vision areas (e.g., image
classification~\cite{krizhevsky2012imagenet} ~\cite{simonyan2014very},
~\cite{szegedy2015going}, object recognition
~\cite{sermanet2013overfeat}~\cite{girshick2014rich}
~\cite{he2014spatial}.  They were also shown capable of per-pixel
tasks, such as semantic segmentation
~\cite{ning2005toward}~\cite{gupta2014learning}, depth estimation from
a single image ~\cite{liu2016learning}, optical flow
estimation~\cite{fischer2015flownet}.

Network architecture is a choice that needs to be made a priori.  To
best of our knowledge, there is no clear guideline on how to choose a
network architecture for a new task.  We experiment with two network architectures:

\begin{itemize}
\item the ZF~\cite{DBLP:journals/corr/ZeilerF13} object recognition
  network.  This is a more traditional CNN architecture.
\item the FlowNet~\cite{fischer2015flownet} optical flow estimation
  network.  This architecture adheres to a newer fully-convolutional
  family of networks.  These are known to train better and have less
  parameters vs. their fully-connected counterpart networks.
\end{itemize}

The input to our network is a pair of subsequent images.  A decision
to make is how to input the images into the network.  There are two
common choices: either create two siamese branches, so that each gets
its own input image, or to create a single multi-channel image.
~\cite{fischer2015flownet} show that the latter works equally well
while being simpler, so we choose to concatenate the images along the
channel dimension to produce a single 6-channel (for color) or
2-channel (for gray-scale) image.

\subsection{ZF}

The network architecture consists of five convolutional and three fully connected layers.  Table~\ref{tab:zf_geometry} summarizes the architecture.

\begin{table}[ht]
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Layer} & \textbf{Receptive Field Size} & \textbf{Padding} & \textbf{Stride} & \textbf{Number of Channels}\\
    \midrule
    conv1&  $7\times 7$& 2& 3&   96\\
    conv2&  $5\times 5$& 2& 2&  256\\
    conv3&  $3\times 3$& 1& 1&  384\\
    conv4&  $3\times 3$& 1& 1&  384\\
    conv5&  $3\times 3$& 1& 1&  256\\
    fc6&               &  &  & 4096\\
    fc7&               &  &  & 4096\\
    fc8&               &  &  &    1\\
  \end{tabular}
  \caption{ZF network geometry}
  \label{tab:zf_geometry}
\end{table}

The flow of data through the network
\begin{figure}
  \begin{tikzpicture}
  [
  data layer/.style={draw=cyan!75, fill=cyan!20, rectangle, minimum width=3cm, minimum height=1.5mm, font=\tiny, transform shape},
  other layer/.style={draw=orange!75, fill=orange!20,rectangle, minimum width=3cm, minimum height=1.5mm, font=\tiny, transform shape},
  text/.style={font=\tiny, rotate=90},
  arrow/.style={->}
  ]
  \draw[rotate=90]
  node[data layer] (dl) {data $6\times 376\times 1241$}
  node[other layer] (l1) [below=.5cm of dl] {conv1 $96\times 125\times 413$}
  node[other layer] (l2) [below=.5cm of l1] {norm1 $96\times 125\times 413$}
  node[other layer] (l3) [below=.5cm of l2] {pool1 $96\times 63\times 207$}
  node[other layer] (l4) [below=.5cm of l3] {conv2 $256\times 30\times 102$}
  node[other layer] (l5) [below=.5cm of l4] {norm2 $256\times 30\times 102$}
  node[other layer] (l6) [below=.5cm of l5] {pool2 $256\times 16\times 52$}
  node[other layer] (l7) [below=.5cm of l6] {conv3 $384\times 14\times 50$}
  node[other layer] (l8) [below=.5cm of l7] {conv4 $384\times 12\times 48$}
  node[other layer] (l9) [below=.5cm of l8] {conv5 $256\times 12\times 48$}
  node[other layer] (l10) [below=.5cm of l9] {fc6 $4096$}
  node[other layer] (l11) [below=.5cm of l10] {fc7 $4096$}
  node[other layer] (l12) [below=.5cm of l11] {fc8 $1$};
  \draw[arrow] (dl) -- (l1);
  \draw[arrow] (l1) -- (l2);
  \draw[arrow] (l2) -- (l3);
  \draw[arrow] (l3) -- (l4);
  \draw[arrow] (l4) -- (l5);
  \draw[arrow] (l5) -- (l6);  
  \draw[arrow] (l6) -- (l7);
  \draw[arrow] (l7) -- (l8);  
  \draw[arrow] (l8) -- (l9);
  \draw[arrow] (l9) -- (l10);  
  \draw[arrow] (l10) -- (l11);
  \draw[arrow] (l11) -- (l12);
  \begin{scope}[on background layer]
    \node[rounded corners, fill=black!20, fit=(dl) (l12)] {};
  \end{scope}
\end{tikzpicture}
\caption{ZF network data flow.  Each rectangle depicts a top blob for a corresponding layer.}
\end{figure}

There is a last fully connected layer that reduces a network to a
single output, which we are interested to learn.  We use euclidean
loss to train the network.  Total number of parameters is about 624
million.


\subsection{FlowNet}

Fully-convolutional network is a recent trend in the field.  They are
known to train better and have less parameters vs. their fully
connected counterparts~\cite{long2015fully}. The geometry of the
network is described in Table~\ref{tab:flownet_geometry}. Each
convolutional layer is followed by the non-linearity.  We chose this
architecture since it proved successful for optical flow estimation
task and has pre-trained model readily available.

\begin{table}[ht]
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Layer} & \textbf{Receptive Field Size} & \textbf{Padding} & \textbf{Stride} & \textbf{Number of Channels}\\
    \midrule
    conv1&   $7\times 7$& 3& 2&   64\\
    conv2&   $5\times 5$& 2& 2&  128\\
    conv3&   $5\times 5$& 2& 2&  256\\
    conv3-1& $3\times 3$& 1& 1&  256\\
    conv4&   $3\times 3$& 1& 2&  512\\
    conv4-1& $3\times 3$& 1& 1&  512\\
    conv5&   $3\times 3$& 1& 2&  512\\
    conv5-1& $3\times 3$& 1& 1&  512\\
    conv6&   $3\times 3$& 1& 2& 1024\\
    conv6-1& $3\times 3$& 1& 1& 1024\\
    conv7&   $3\times 3$& 1& 2&    1\\
    \hline
  \end{tabular}
  \caption{Geometry of the FlowNet based network}
  \label{tab:flownet_geometry}
\end{table}

\begin{tikzpicture}
  [
  data layer/.style={draw=cyan!75, fill=cyan!20, rectangle, minimum width=3cm, minimum height=1.5mm, font=\tiny, transform shape},
  other layer/.style={draw=orange!75, fill=orange!20,rectangle, minimum width=3cm, minimum height=1.5mm, font=\tiny, transform shape},
  text/.style={font=\tiny, rotate=90},
  arrow/.style={->}
  ]
  \draw[rotate=90]
  node[data layer] (dl) {data $6\times 376\times 1241$}
  node[other layer] (l1) [below=.5cm of dl] {data $6\times 376\times 1241$}
  node[other layer] (l2) [below=.5cm of l1] {conv1 $64\times 188\times 621$}
  node[other layer] (l3) [below=.5cm of l2] {conv2 $128\times 94\times 311$}
  node[other layer] (l4) [below=.5cm of l3] {conv3 $256\times 47\times 156$}
  node[other layer] (l5) [below=.5cm of l4] {conv3-1 $256\times 47\times 156$}
  node[other layer] (l6) [below=.5cm of l5] {conv4 $512\times 24\times 78$}
  node[other layer] (l7) [below=.5cm of l6] {conv4-1 $512\times 24\times 78$}
  node[other layer] (l8) [below=.5cm of l7] {conv5 $512\times 12\times 39$}
  node[other layer] (l9) [below=.5cm of l8] {conv5-1 $512\times 12\times 39$}
  node[other layer] (l10) [below=.5cm of l9] {conv6 $1024\times 6\times 20$}
  node[other layer] (l11) [below=.5cm of l10] {conv6-1 $1024\times 6\times 20$}
  node[other layer] (l12) [below=.5cm of l11] {conv7 $1\times 6\times 20$}
  node[other layer] (l13) [below=.5cm of l12] {scale $1\times 1\times 1$};
  \draw[arrow] (dl) -- (l1);
  \draw[arrow] (l1) -- (l2);
  \draw[arrow] (l2) -- (l3);
  \draw[arrow] (l3) -- (l4);
  \draw[arrow] (l4) -- (l5);
  \draw[arrow] (l5) -- (l6);  
  \draw[arrow] (l6) -- (l7);
  \draw[arrow] (l7) -- (l8);  
  \draw[arrow] (l8) -- (l9);
  \draw[arrow] (l9) -- (l10);  
  \draw[arrow] (l10) -- (l11);
  \draw[arrow] (l11) -- (l12);
  \draw[arrow] (l12) -- (l13);
  \begin{scope}[on background layer]
    \node[rounded corners, fill=black!20, fit=(dl) (l13)] {};
  \end{scope}
\end{tikzpicture}

We use Euclidean loss to train the network. The total number of
parameters is 24 million.

\section{Recurrent Neural Networks}

Many problems require passing information through time.  Since the
traditional neural networks are stateless, they are poorly suited for
this purpose. Recurrent Neural Networks have loops in them, so that
the information can persist.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    [
    ->,
    >=stealth',
    auto,node distance=1.5cm,
    thick,
    main node/.style={circle, draw, font=\sffamily\large\bfseries},
    rect node/.style={rectangle, draw, font=\sffamily\large\bfseries}
    ]
    \node[main node] (1) {$x_t$};
    \node[rect node] (2) [above of=1, minimum width=1.5cm] {$A$};
    \node[main node] (3) [above of=2] {$h_t$};
    
    \path[every node/.style={font=\sffamily\small}]
    (1) edge node [right] {} (2)
    (2) edge node [right] {} (3);
    \path (2) edge [out=45,in=135,looseness=6] node[below] {} (2);
  \end{tikzpicture}
  \caption{RNN}
  \label{fig:rnn1}  
\end{figure}

In the Figure~\ref{fig:rnn1} network $A$ accepts input $x_t$ and
outputs $h_t$.  The loop allows the network to pass information from
one step of to another.

The RNN may be thought of as a chain of multiple copies of the same
neural network where each node passes a message to its successor.
This is called \textit{network unrolling}, depicted in
Figure~\ref{fig:rnn2}.  Unrolled network stresses the relation of the
RNN to the sequential data streams.  This is an architecture of choice
when modeling such data.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    [
    ->,
    >=stealth',
    auto,node distance=1.5cm,
    thick,
    main node/.style={circle, draw, font=\sffamily\large\bfseries},
    rect node/.style={rectangle, draw, font=\sffamily\large\bfseries}
    ]
    \node[main node] (1) {$x_t$};
    \node[rect node] (2) [above of=1, minimum width=1.5cm] {$A$};
    \node[main node] (3) [above of=2] {$h_t$};
    
    \path[every node/.style={font=\sffamily\small}]
    (1) edge node [right] {} (2)
    (2) edge node [right] {} (3);
    \path (2) edge [out=45,in=135,looseness=8] node[below] {} (2);
    \node[main node] (4) [right = 2cm of 1] {$x_0$};
    \node[rect node] (5) [above of=4, minimum width=1.5cm] {$A$};
    \node[main node] (6) [above of=5] {$h_0$};
    \path[every node/.style={font=\sffamily\small}]
    (4) edge node [right] {} (5)
    (5) edge node [right] {} (6);

    \node[main node] (7) [right = 1cm of 4] {$x_1$};
    \node[rect node] (8) [above of=7, minimum width=1.5cm] {$A$};
    \node[main node] (9) [above of=8] {$h_1$};
    \path[every node/.style={font=\sffamily\small}]
    (7) edge node [right] {} (8)
    (8) edge node [right] {} (9)
    (5) edge node [right] {} (8);
    \node (10) [right = .5 cm of 2] {=};
    \node[main node] (11) [right = 1cm of 7] {$x_2$};
    \node[rect node] (12) [above of=11, minimum width=1.5cm] {$A$};
    \node[main node] (13) [above of=12] {$h_2$};
    \path[every node/.style={font=\sffamily\small}]
    (11) edge node [right] {} (12)
    (12) edge node [right] {} (13)
    (8) edge node [right] {} (12);
    \node (14) [right = .2 cm of 12] {$\cdots$};
    (15) edge node [right] {} (16)
    (16) edge node [right] {} (17)
    (17) edge node [right] {} (18);
    \node[main node] (15) [right = 2cm of 11] {$x_t$};
    \node[rect node] (16) [above of=15, minimum width=1.5cm] {$A$};
    \node[main node] (17) [above of=16] {$h_t$};
    \path[every node/.style={font=\sffamily\small}]
    (15) edge node [right] {} (16)
    (16) edge node [right] {} (17);
  \end{tikzpicture}
  \caption{Unrolled Recurrent Neural Network}
  \label{fig:rnn2}
\end{figure}

It turns out that the vanilla RNN are hard to train due to the
exploding/imploding gradients.  Fortunately, Long Short Term Memory
Networks (LSTM), which is a special kind of RNN addresses these issues.

LSTM were introduced by~\cite{hochreiter1997long}, they are
specifically designed to model long term dependencies. Similarly to
RNN, LSTM posses a chain-like structure. Each repeating node has an
internal structure depicted in Figure~\ref{fig:lstm1}.

The first step in LSTM is to decide what information we are going to
make use of in the cell state $C_{t-1}$.  This is done by a sigmoid
layer ``forget gate layer'', equation ~\ref{eq:forget gate}, that
looks at the previous output $h_{t-1}$ and the current input $x_t$ and
outputs a number in $[0,1]$ for every cell state component. $0$ means
completely forget this component and $1$ means completely make use of
it.
\begin{equation}\label{eq:forget gate}
  f_t = \sigma(W_f[h_{t-1}, x_t] + b_f)
\end{equation}
Next, LSTM makes a step toward the cell state update. The ``input gate
layer'' outputs a number in $[0,1]$ for each cell state component
(similarly to the forget gate, see equation~\ref{eq:input gate}) and
the ``tanh'' layer computes a cell state update candidate vector
(equation~\ref{eq:update candidate}):

\begin{equation}\label{eq:input gate}
  i_t = \sigma(W_i[h_{t-1},x_t] + b_i)
\end{equation}
\begin{equation}\label{eq:update candidate}
  \tilde{C_t} = tanh(W_C[h_{t-1},x_t] + b_C)
\end{equation}

Cell state is updated by ``forgetting'' some of the information (as
decided by the ``forget gate'') and by adding some new information
(governed by the ``input gate'' and the ``tanh''):
\begin{equation}
  C_t = f_t*C_{t-1}+i_t*\tilde{C_t}
\end{equation}
Now LSTM decides on its output.  The output is based on the cell state
\begin{equation}
  o_t = \sigma(W_o[h_{t-1},x_t]+b_o)
\end{equation}
\begin{equation}
  h_t = o_t*tanh(C_t)
\end{equation}

\begin{figure}[ht!]
  \centering \includegraphics[width=.7\textwidth]{LSTM3-chain}
  \caption{Long Short Term Memory}\label{fig:lstm1}
\end{figure}

\begin{figure}[ht!]
  \centering \includegraphics[width=.5\textwidth]{LSTM2-notation}
  \caption{A notation used in Figure~\ref{fig:lstm1}}\label{fig:lstm2}
\end{figure}
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    [
    auto,node distance=1.5cm,
    thick,
    circ node/.style={circle, draw=black!75, minimum size=2mm, font=\tiny, },
    rect node/.style={rectangle, draw=black!75, minimum size=2mm, font=\tiny},
    ]
    \node[rect node] (1) {$\sigma$};
    \node[rect node] (2) [right=1cm of 1] {$\sigma$};
    \node[rect node] (3) [right=1cm of 2]{$tanh$};
    \node[rect node] (4) [right=1cm of 3] {$\sigma$};
    \node[circ node] (5) [above=1.5cm of 1] {$\times$};
    \node[circ node] (6) [above=.35cm of 3] {$\times$};
    \node[circ node] (7) [above=1.5cm of 3] {+};
    \node[circ node] (8) [below=1cm of 1] {$x_t$};
    \node[circ node] (9) [right=2cm of 6] {$\times$};
    \node (10) [rectangle, rounded corners=1ex, draw, above=.15cm of 9, font=\tiny, draw=black!75] {$tanh$};
    \node (11) [right=4cm of 7] {$C_t$};
    \node (15) [above=3.1mm of 8, inner sep=0mm] {};    
    \node (12) [right=7cm of 15] {$h_t$};
    \node (13) [left=1cm of 5] {$C_{t-1}$};
    \node (14) [left=1cm of 15] {$h_{t-1}$};
    \begin{scope}[->,decoration={post length=4pt},rounded corners=2mm]
      \draw[--] (8) -- (1);
      \draw (15) -- ++(.5cm, 0) -| (2.south);
      \draw (15) -- ++(2cm, 0) -| (3.south);
      \draw (15) -- ++(3cm, 0) -| (4.south);
      \draw (1) -- (5);
      \draw (5) -- (7);
      \draw (3) -- (6);
      \draw (2.north) -- ++(0,.35cm) |- (6.west);
      \draw (6) -- (7);
      \draw (7) -- (11);
      \draw (4.north) -- ++(0,3.5mm) |- (9.west);
      \draw (10) -- (9);
      \draw (7) -| (10.north);
      \draw (9.south) |- (12.west);
      \draw (13) -- (5);
      \draw (14) -- (15);
    \end{scope}
    \begin{scope}[on background layer]
      \node[rounded corners, fill=black!20, fit=(2) (3) (4) (5) (6) (7) (9) (10) (15)] {};
    \end{scope}
  \end{tikzpicture}
  \caption{LSTM Node}
  \label{fig:lstm node}
\end{figure}

We combine the sequential strengths of the LSTM with convolutional
networks to tackle our problem.  $x_t$ vectors are produced by a
convolutional network.  This is a fairly common representation used
previously.  The convnet and the LSTM are trained jointly.

The geometry of the network is as follows:
\begin{itemize}
\item conv1: (96, 6, 7, 7)
\item conv2: (384, 48, 5, 5)
\item conv3: (512, 384, 3, 3)
\item conv4: (512, 256, 3, 3)
\item conv5: (384, 256, 3, 3)
\item fc6:   (4096, 34560)
\item lstm1: (1024, 4096)
\item fc-final: (1, 256)
\end{itemize}
Total number of trainable parameters is about 150 millions.

\section{Experiments}

\subsection{Data-set}

We train and test on the KITTI data-set~\cite{geiger2013vision}.  The
data-set consists of 11 sequences with ground truth data.  We
arbitrarily use sequence 00 for testing and sequences 01-10 for
training.  There are 4540 and 18650 images in the test and the train
sets respectively. We only use the data from the left color camera.
Figure~\ref{fig:scales} depicts the distribution of the scale values
over the train and the test sets.

\begin{figure}[!ht]
  \includegraphics[width=0.9\linewidth]{no_00-scales}
  \caption{The train set}
  \label{fig:stats-no_00}
  \caption{Scale distributions}
  \label{fig:scales}
\end{figure}

\subsection{Random Forest}

\subsection{Feature Extraction}\label{sec:features}

In order to train the random forest we need to represent subsequent
image pairs as feature vectors, which are created as follows: we
extract and match sparse salient points in both input images.  The
output of this stage is a set of a corresponding pixel locations.
Then, we compute the point displacement magnitudes.  Finally, we bin
all the interest pundits according to a grid and create histogram of
displacement magnitudes for each bin.  Concatenating all histograms
together produces a feature vector. We use Harris corners and square
$11\times 11$ patches as corner descriptors. Sum of square differences
is used as a distance measure with the winning pair declared a match.
To prune the outliers we fit the fundamental matrix into the matched
corner sets and remove the corners that do not agree with the model.
Figure~\ref{fig:ex_corner_and_matching} shows a typical example of
extracted and matched corners.


Some statistics of the features is presented in the
Figure~\ref{fig:feature_vectors}. We expect the peaks, that correspond
to a closer image regions have a distributions shifted to the right
(i.e., larger displacements) and the peaks that correspond to a
regions farther away should be closer to zero.  This behavior can be
observed especially well for the feature vectors that correspond to
larger camera displacements (e.g. Figure~\ref{fig:1c}).

\begin{figure}[!ht]
  \centering
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{10_001157_001158_raw_corners_left}
    \caption{}
    \label{fig:ex_corner_and_matching:corner}
  \end{subfigure}
  \begin{subfigure}{.45\linewidth}
    \includegraphics[width=\linewidth]{10_001157_001158_final_matches1}
    \caption{}
    \label{fig:ex_corner_and_matching:match}    
  \end{subfigure}
  \caption{Typical corner extraction and
    matching. Figure~\subref{fig:ex_corner_and_matching:corner} shows the raw
    extracted corners, while Figure~\subref{fig:ex_corner_and_matching:match} shows
    pruned and matched corners.}
  \label{fig:ex_corner_and_matching}
\end{figure}

\begin{figure}[!ht]
  \centering
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics[width=1.2\linewidth]{00_mean_feature_vector_0_47}
    \caption{}\label{fig:1a}
  \end{subfigure}%
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics[width=1.2\linewidth]{00_mean_feature_vector_0_81}
    \caption{}\label{fig:1b}
  \end{subfigure}%
  \\
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=.8\textwidth]{00_mean_feature_vector_1_1}
    \caption{}\label{fig:1c}
  \end{subfigure}%
  \caption{Average feature vectors for samples centered around the
    specific camera translation magnitude.  Each peak corresponds to a
    grid cell (e.g. here the grid is 4 rows by 6 columns by 300 bins,
    so the feature vector is of the dimension 6*4*300=7200).  The grid
    is sampled in a column-major mode. So the first four peaks
    correspond to the leftmost column of the image grid.}
  \label{fig:feature_vectors}
\end{figure}

We bin each image into $6\times 4$ grid.  For each bin in the image we
compute the histogram of corner disparities.  By disparity we denote
the displacement of the corner in the image.  We use $300$-bin
histogram for disparities (e.g, feature vector length is $7200$).

We use the extracted features to fit a random forest, by means of
recursive note splitting with sub-node variance minimization. The mean
absolute error is 0.174 meters with a standard deviation of 0.148
meters.

\section{Neural networks}

We use Caffe~\cite{jia2014caffe} framework to train and test our
models.
\paragraph{ZF} We train using vanilla SGD for 100 epochs and decrease
learning rate by factor of .1 every 10 epochs. We train the network
from scratch.
\paragraph{FlowNet} We also train using vanilla SGD for 100 epochs
decreasing learning rate by .1 every 10 epochs.  We start from
pre-trained weights of~\cite{fischer2015flownet}.
\paragraph{LSTM} TBD

\section{Results}

Let the ground truth sequence of camera motion scales by
$\{y_i\}_{i=0}^N$.  Let the predicted scales for the same sequence be
$\{\hat{y}_i\}_{i=0}^N$.  We report the mean absolute error:
\[
  \mu = \frac{1}{N}\sum_0^N{|y_i - \hat{y}_i|}
\]

\begin{table}[ht]
  \centering
  \begin{tabular}{ lcccc }
    \hline
                        & Random Forest & ZF   & FlowNet & LSTM ZF \\
    \hline
    $\mu\quad[meters]$        & .237          & .200 & \textbf{.103}    & .198 \\
    $\sigma\quad[meters]$     & .229          & .161 & \textbf{.08}     & .147 \\    
    \hline
  \end{tabular}
  \caption{Experimental Results}
  \label{table:1}
\end{table}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
