

\chapter{Introduction}
\label{chap:intro}

Visual odometry refers to the problem of recovering camera motion
based on the images taken by it. This problem naturally occurs in
robotics, wearable computing, augmented reality and automotive.

Wheel odometry, recovers the motion of the vehicle by examining and
integrating the wheel turns over time.  In a similar manner, visual
odometry operates by estimating relative motion of the camera between
subsequent images by observing changes in them. Later, these estimates
are combined into a single trajectory. Just as in wheel odometry,
visual odometry is subject to error accumulation over time. Contrary
to wheel odometry, visual odometry is not affected by wheel slip in a
rough terrain. Visual odometry is able to produce motion estimates
with errors that are lower than those of the wheel odometry. Another
advantage of visual odometry is that cameras are low cost and low
weight sensors.  All these make visual odometry a viable supplement to
other motion recover methods such as global positioning systems (GPS)
and inertial measurement units (IMUs).

Visual odometry becomes a harder problem as the amount of detail in
the images diminishes. The images should have sufficient overlap and
the scene needs to be illuminated.  In the stereo setup, the scene
must be static or the images taken at the same time. Also, the video
processing incurs a computational burden.

Visual odometry is an active fields of research with a large amount of
published work.  We review only the most pertinent works.
~\cite{Scaramuzza2011} provides a more complete survey.

Similar to~\cite{Persson2015} we partition visual odometry algorithms
by four traits:
\begin{enumerate}
\item Feature-based vs direct
\item Global vs local
\item Filter based vs bundle adjustment based
\item Monocular vs stereo
\end{enumerate}

Visual odometry algorithms use large number of corner detectors
(e.g., Moravec~\cite{Moravec1980}, Harris~\cite{Harris1987},
Shi-Tomasi~\cite{Shi1994}, Fast~\cite{Rosten2006}) and blob detectors
(e.g., SIFT~\cite{Lowe2004}, SURF~\cite{Bay2006}). Corners are faster to
compute and usually are better localized, while blobs are more robust
to scale change. The choice of a specific feature point depends mainly
on the images at hand.  Motion estimation results for different
feature points are presented in~\cite{Govender2009}. In this work we
choose Harris~\cite{Harris1987} corners, but this choice is not
crucial. We view the feature point choice as a parameter, which needs
to be determined from the data (e.g., by cross-validation).

The features are either tracked~\cite{Hedborg2009} or
matched~\cite{Geiger2011} (i.e., freshly detected in each new frame)
between subsequent images. While the early works chose to track
features, most of the current works detect and match them. The output
of this stage are pairs of the image features, which are the
projections of the same 3-D point.

Matched features are used as an input for a motion estimation
procedure.  Whether the features are specified in 2-D or 3-D, the
estimation procedures, may be classified into 3-D-to-3-D
~\cite{Milella2006}, 3-D-to-2-D ~\cite{Geiger2011} and 2-D-to-2-D
~\cite{Nister2004}. Most of the early works were of the 3-D-to-3-D
type.  More recent works~\cite{Nister2004} claim that this approach is
inferior to the latter two. Popular techniques that participate in
most algorithms in some way are essential matrix estimation and
(possibly) its subsequent decomposition~\cite{Nister2004}, perspective
3-point algorithm~\cite{Kneip1991}, and re-projection error
minimization~\cite{Geiger2011}.

Global methods~\cite{Klein2007},~\cite{Newcombe2011} keep the map of
the environment and make sure that motion estimates are globally
consistent with this map, while local methods do not.  Some local
methods~\cite{Badino2013} also keep track of a (local) map, but the
underlying philosophy is different: global vs local.  Global methods
usually more accurate since they make use of a vast amount of
information (which, of course, comes at a computational price).  Note
that accuracy does not imply robustness, since outliers that made
their way into the map may greatly skew subsequent pose estimates.

Methods that explicitly model system state uncertainty tend to use
filtering mathematical machinery,
e.g.,~\cite{Konolige2010},~\cite{Olson2003},~\cite{Kaess2008}.
Another alternative to maintain map/pose estimate consistency is to
use the bundle adjustment approach~\cite{Triggs2000}. Monocular
systems~\cite{Song} make use of a single camera, while stereo
systems~\cite{Geiger2011} rely on a calibrated stereo rig. In the
monocular setup the translation of the camera may only be estimated up
to scale, while in stereo all six motion parameters may be
recovered. An additional advantage of the stereo setup is that more
information is available at each step, which may be one of the reasons
why stereo algorithms perform better.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../"
%%% End:
