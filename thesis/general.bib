@Book{Hartley2004, 
    author = "Hartley, R.~I. and Zisserman, A.",
    title = "Multiple View Geometry in Computer Vision",
    edition = "Second",
    year = "2004",
    publisher = "Cambridge University Press, ISBN: 0521540518"
}
@article{Agrawal2008,
author = {Agrawal, M and Konolige, K and Blas, M R},
journal = {Eccv08},
pages = {IV: 102--115},
title = {{CenSurE: Center Surround Extremas for Realtime Feature Detection and Matching}},
year = {2008}
}
@article{Badino2013,
author = {Badino, Hern{\'{a}}n and Yamamoto, Akihiro and Kanade, Takeo},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {Cvad 13},
pages = {222--229},
title = {{Visual odometry by multi-frame feature integration}},
year = {2013}
}
@article{Bay2006,
abstract = {Abstract. In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Ro- bust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {404--417},
title = {{SURF: Speeded up robust features}},
volume = {3951 LNCS},
year = {2006}
}
@article{Civera,
author = {Civera, Javier and Davison, Andrew J},
title = {{Unified Inverse Depth Parametrization for Monocular SLAM}},
}
@article{Geiger2012,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3354--3361},
title = {{Are we ready for autonomous driving? the KITTI vision benchmark suite}},
year = {2012}
}
@article{Geiger2011,
abstract = {Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed. This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.},
author = {Geiger, Andreas and Ziegler, Julius and Stiller, Christoph},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {963--968},
title = {{StereoScan: Dense 3d reconstruction in real-time}},
year = {2011}
}
@article{Govender2009,
abstract = {3rd Robotics and Mechatronics Symposium (ROBMECH 2009). Pretoria, South Africa, 8-10 November 2009},
author = {Govender, Natasha},
journal = {Csir},
title = {{Evaluation of Feature Detection Algorithms for Structure from Motion}},
year = {2009}
}
@article{Harris1987,
author = {Harris, C G and Pike, J M},
journal = {Procedings of the Alvey Vision Conference 1987},
pages = {233--236},
title = {{3D Positional Integration from Image Sequences}},
year = {1987}
}
@article{Harris1988,
abstract = {Consistency of image edge filtering is of prime importance for 3D interpretation of image sequences using feature tracking algorithms. To cater for image regions containing texture and isolated features, a combined corner and edge detector based on the local auto-correlation function is utilised, and it is shown to perform with good consistency on natural imagery.},
author = {Harris, Chris and Stephens, Mike},
journal = {Procedings of the Alvey Vision Conference 1988},
pages = {147--151},
title = {{A Combined Corner and Edge Detector}},
year = {1988}
}
@article{Hedborg2009,
author = {Hedborg, Johan and Forss{\'{e}}n, Pe and Felsberg, Michael},
journal = {5th International Symposium on Advances in Visual Computing: Part I},
pages = {211--222},
title = {{Fast and accurate structure and motion estimation}},
year = {2009}
}
@article{Horn1987,
abstract = {Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task. It finds applications in stereophotogrammetry and in robotics. I present here a closed-form solution to the least-squares problem for three or more points. Currently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is simplified by use of unit quaternions to represent rotation. I emphasize a symmetry property that a solution to this problem ought to possess. The best translational offset is the difference between the centroid of the coordinates in one system and the rotated and scaled centroid of the coordinates in the other system. The best scale is equal to the ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids. These exact results are to be preferred to approximate methods based on measurements of a few selected points. The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue of a symmetric 4 Ã— 4 matrix. The elements of this matrix are combinations of sums of products of corresponding coordinates of the points.},
author = {Horn, Berthold K P},
journal = {Journal of the Optical Society of America A},
number = {4},
pages = {629},
title = {{Closed-form solution of absolute orientation using unit quaternions}},
volume = {4},
year = {1987}
}
@article{Kaess2008,
abstract = {In this paper, we present incremental smoothing and mapping (iSAM), which is a novel approach to the simultaneous localization and mapping problem that is based on fast incremental matrix factorization. iSAM provides an efficient and exact asolution by updating a QR factorization of the naturally sparse smoothing information matrix, thereby recalculating only those matrix entries that actually change. iSAM is efficient even for robot trajectories with many loops as it avoids unnecessary fill-in in the factor matrix by periodic variable reordering. Also, to enable data association in real time, we provide efficient algorithms to access the estimation uncertainties of interest based on the factored information matrix. We systematically evaluate the different components of iSAM as well as the overall algorithm using various simulated and real-world datasets for both landmark and pose-only settings.},
author = {Kaess, Michael and Ranganathan, Ananth and Dellaert, Frank},
number = {6},
pages = {1365--1378},
title = {{iSAM: Incremental smoothing and mapping}},
volume = {24},
year = {2008}
}
@article{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR},
title = {{Parallel tracking and mapping for small AR workspaces}},
year = {2007}
}
@article{Kneip1991,
abstract = {The Perspective-Three-Point (P3P) problem aims at determining the position and orientation of the camera in the world reference frame from three 2D-3D point correspondences. This problem is known to provide up to four solutions that can then be disambiguated using a fourth point. All existing solutions attempt to first solve for the position of the points in the camera reference frame, and then compute the position and orientation of the camera in the world frame, which alignes the two point sets. In contrast, in this paper we propose a novel closed-form solution to the P3P problem, which computes the aligning transformation directly in a single stage, without the intermediate derivation of the points in the camera frame. This is made possible by introducing intermediate camera and world reference frames, and expressing their relative position and orientation using only two parameters. The projection of a world point into the parametrized camera pose then leads to two conditions and finally a quartic equation for finding up to four solutions for the parameter pair. A subsequent backsubstitution directly leads to the corresponding camera poses with respect to the world reference frame. We show that the proposed algorithm offers accuracy and precision comparable to a popular, standard, state-of-the-art approach but at much lower computational cost (15 times faster). Furthermore, it provides improved numerical stability and is less affected by degenerate configurations of the selected world points. The superior computational efficiency is particularly suitable for any RANSAC-outlier-rejection step, which is always recommended before applying PnP or non-linear optimization of the final solution.},
author = {Kneip, Laurent and Scaramuzza, Davide and Siegwart, Roland},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2969--2976},
title = {{A novel parametrization of the perspective-three-point problem for a direct computation of absolute camera position and orientation}},
year = {2011}
}
@article{Konolige2010,
abstract = {Motion estimation from imagery, sometimes called visual odometry, is a well-known process. However, it is difficult to achieve good performance using standard techniques. In this paper, we present the results of several years of work on an integrated system to localize a mobile robot in rough outdoor terrain using visual odometry, with an increasing degree of precision. We discuss issues that are important for realtime, high-precision performance: choice of features, matching strategies, incremental bundle adjustment, and filtering with inertial measurement sensors. Using data with ground truth from an RTK GPS system, we show experimentally that our algorithms can track motion, in off-road terrain, over distances of 10 km, with an error of less than 10 m (0.1{\%}).},
author = {Konolige, Kurt and Agrawal, Motilal and Sol{\`{a}}, Joan},
journal = {Springer Tracts in Advanced Robotics},
number = {STAR},
pages = {201--212},
title = {{Large-scale visual odometry for rough terrain}},
volume = {66},
year = {2010}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G},
journal = {Int'l Journal of Computer Vision},
pages = {91--11020042},
title = {{Distinctive image features from scale invariant keypoints}},
volume = {60},
year = {2004}
}
@article{Milella2006,
abstract = { In this paper, we present a stereovision algorithm for real-time 6DoF ego-motion estimation, which integrates image intensity information and 3D stereo data in the well-known Iterative Closest Point (ICP) scheme. The proposed method addresses a basic problem of standard ICP, i.e. its inability to perform the segmentation of data points and to deal with large displacements. Neither a-priori knowledge of the motion nor inputs from other sensors are required, while the only assumption is that the scene always contains visually distinctive features which can be tracked over subsequent stereo pairs. This generates what is usually called Visual Odometry. The paper details the various steps of the algorithm and presents the results of experimental tests performed with an allterrain mobile robot, proving the method to be as accurate as effective for autonomous navigation purposes.},
author = {Milella, Annalisa and Siegwart, Roland},
journal = {Proceedings of the Fourth IEEE International Conference on Computer Vision Systems, ICVS'06},
number = {Icvs},
pages = {21},
title = {{Stereo-based ego-motion estimation using pixel tracking and iterative closest point}},
volume = {2006},
year = {2006}
}
@article{Moravec1980,
abstract = {The Stanford Al Lab cart is a card-table sized mobile robot controlled remotely through a radio link, and equipped with a TV camera and transmitter. A computer has been programmed to drive the cart through cluttered indoor and outdoor spaces, gaining its knowledge of the world entirely from images broadcast by the onboard TV system. The cart uses several kinds of stereo to locate objects around it in 3D and to deduce its own motion. It plans an obstacle avoiding path to a desired destination on the basis of a model built with this information. The plan changes as the cart perceives new obstacles on its journey . The system is reliable for short runs, but slow. The cart moves one meter every ten to fifteen minutes, in lurches. After rolling a meter it stops, takes some pictures and thinks about them for a long time. Then it plans a new path, executes a little of it, and pauses again. The program has successfully driven the cart through several 20 meter indoor courses (each taking about five hours) complex enough to necessitate three or four avoiding swerves. A less successful outdoor run, in which the cart skirted two obstacles but collided with a third, was also done. Harsh lighting (very bright surfaces next to very dark shadows) giving poor pictures and movement of shadows during the cart's creeping progress were major reasons for the poorer outdoor performance. The action portions of these runs were filmed by computer controlled cameras.},
author = {Moravec, Hans Peter},
journal = {tech. report CMU-RI-TR-80-03},
pages = {175},
title = {{Obstacle avoidance and navigation in the real world by a seeing robot rover.}},
year = {1980}
}
@article{Newcombe2011,
author = {Newcombe, R A and Lovegrove, S J and Davison, A J},
journal = {Int. Conf. on Computer Vision (ICCV)},
pages = {2320--2327},
title = {{{\{}DTAM{\}}: Dense Tracking and Mapping in Real-Time}},
year = {2011}
}
@article{Nister2004,
abstract = {An efficient algorithmic solution to the classical five-point relative pose problem is presented. The problem is to find the possible solutions for relative camera pose between two calibrated views given five corresponding points. The algorithm consists of computing the coefficients of a tenth degree polynomial in closed form and, subsequently, finding its roots. It is the first algorithm well-suited for numerical implementation that also corresponds to the inherent complexity of the problem. We investigate the numerical precision of the algorithm. We also study its performance under noise in minimal as well as overdetermined cases. The performance is compared to that of the well-known 8 and 7-point methods and a 6-point scheme. The algorithm is used in a robust hypothesize-and-test framework to estimate structure and motion in real-time with low delay. The real-time system uses solely visual input and has been demonstrated at major conferences.}
author = {Nist{\'{e}}r, David},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Camera calibration,Ego-motion estimation,Imaging geometry,Motion,Relative orientation,Scene reconstruction,Structure from motion},
number = {6},
pages = {756--770},
title = {{An efficient solution to the five-point relative pose problem}},
volume = {26},
year = {2004}
}
@article{Olson2003,
abstract = {Robust navigation for mobile robots over long distances requires an accurate method for tracking the robot position in the environment. Promising techniques for position estimation by determining the camera ego-motion from monocular or stereo sequences have been previously described. However, long-distance navigation requires both a high level of robustness and a low rate of error growth. In this paper, we describe a methodology for long-distance rover navigation that meets these goals using robust estimation of ego-motion. The basic method is a maximum-likelihood ego-motion algorithm that models the error in stereo matching as a normal distribution elongated along the (parallel) camera viewing axes. Several mechanisms are described for improving navigation robustness in the context of this methodology. In addition, we show that a system based on only camera ego-motion estimates will accumulate errors with super-linear growth in the distance traveled, owing to increasing orientation errors. When an absolute orientation sensor is incorporated, the error growth can be reduced to a linear function of the distance traveled. We have tested these techniques using both extensive simulation and hundreds of real rover images and have achieved a low, linear rate of error growth. This method has been implemented to run on-board a prototype Mars rover. ?? 2003 Elsevier Science B.V. All rights reserved.},
author = {Olson, Clark F. and Matthies, Larry H. and Schoppers, Marcel and Maimone, Mark W.},
journal = {Robotics and Autonomous Systems},
keywords = {Mars rovers,Motion estimation,Robot navigation,Stereo vision},
number = {4},
pages = {215--229},
title = {{Rover navigation using stereo ego-motion}},
volume = {43},
year = {2003}
}
@article{Persson2015,
abstract = {Visual odometry is one of the most active topics in computer vision. The automotive industry is particularly interested in this field due to the appeal of achieving a high degree of accuracy with inexpensive sensors such as cameras. The best results on this task are currently achieved by systems based on a calibrated stereo camera rig, whereas monocular systems are generally lagging behind in terms of performance. We hypothesise that this is due to stereo visual odometry being an inherently easier problem, rather than than due to higher quality of the state of the art stereo based algorithms. Under this hypothesis, techniques developed for monocular visual odometry systems would be, in general, more refined and robust since they have to deal with an intrinsically more difficult problem. In this work we present a novel stereo visual odometry system for automotive applications based on advanced monocular techniques. We show that the generalization of these techniques to the stereo case result in a significant improvement of the robustness and accuracy of stereo based visual odometry. We support our claims by the system results on the well known KITTI benchmark, achieving the top rank for visual only systems},
author = {Persson, Mikael and Piccini, Tommaso and Felsberg, Michael and Mester, Rudolf},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {686--691},
title = {{Robust stereo visual odometry from monocular techniques}},
volume = {2015-Augus},
year = {2015}
}
@article{Pollefeys2008,
abstract = {Page 1. Int J Comput Vis (2008) 78: 143â€“167 DOI 10.1007 / s11263 - 007 - 0086 - 4 Detailed Real-Time Urban 3D Reconstruction from Video M. Pollefeys {\textperiodcentered} D. Nist{\'{e}}r {\textperiodcentered} J.-M. Frahm {\textperiodcentered} A. Akbarzadeh {\textperiodcentered} P. Mordohai {\textperiodcentered} B. Clipp {\textperiodcentered} C. Engels ... $\backslash$n},
author = {Pollefeys, M and Nist{\'{e}}r, D and Frahm, Jm},
journal = {International Journal of {\ldots}},
keywords = {3d reconstruction,depth map fusion,from motion,large scale modeling,plane sweeping,reconstruction,stereo vision,structure,urban},
pages = {1--43},
title = {{Detailed real-time urban 3d reconstruction from video}},
year = {2008}
}
@article{Rosten2006,
abstract = {Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7{\%} of the available processing time. By comparison neither the Harris detector (120{\%}) nor the detection stage of SIFT (300{\%}) can operate at full frame rate. Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.},
author = {Rosten, Edward and Drummond, Tom},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {430--443},
title = {{Machine learning for high-speed corner detection}},
volume = {3951 LNCS},
year = {2006}
}
@article{Scaramuzza2011a,
abstract = {Visual odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human, and robot) using only the input of a single or If multiple cameras attached to it. Application domains include robotics, wearable computing, augmented reality, and automotive. The term VO was coined in 2004 by Nister in his landmark paper. The term was chosen for its similarity to wheel odometry, which incrementally estimates the motion of a vehicle by integrating the number of turns of its wheels over time. Likewise, VO operates by incrementally estimating the pose of the vehicle through examination of the changes that motion induces on the images of its onboard cameras. For VO to work effectively, there should be sufficient illumination in the environment and a static scene with enough texture to allow apparent motion to be extracted. Furthermore, consecutive frames should be captured by ensuring that they have sufficient scene overlap.},
author = {Scaramuzza, Davide and Fraundorfer, Friedrich},
journal = {IEEE Robotics {\&} Automation Magazine},
number = {4},
pages = {80--92},
title = {{Visual Odometry [Tutorial]}},
volume = {18},
year = {2011}
}
@article{Scaramuzza2011,
author = {Scaramuzza, Davide and Fraundorfer, Friedrich},
number = {June},
title = {{Visual Odometry}},
year = {2011}
}
@article{Scaramuzza2009,
abstract = {In structure-from-motion with a single camera it is well known that the scene can be only recovered up to a scale. In order to compute the absolute scale, one needs to know the baseline of the camera motion or the dimension of at least one element in the scene. In this paper, we show that there exists a class of structure-from-motion problems where it is possible to compute the absolute scale completely automatically without using this knowledge, that is, when the camera is mounted on wheeled vehicles (e.g. cars, bikes, or mobile robots). The construction of these vehicles puts interesting constraints on the camera motion, which are known as {\&}{\#}x201C;nonholonomic constraints{\&}{\#}x201D;. The interesting case is when the camera has an offset to the vehicle's center of motion. We show that by just knowing this offset, the absolute scale can be computed with a good accuracy when the vehicle turns. We give a mathematical derivation and provide experimental results on both simulated and real data over a large image dataset collected during a 3 Km path. To our knowledge this is the first time nonholonomic constraints of wheeled vehicles are used to estimate the absolute scale. We believe that the proposed method can be useful in those research areas involving visual odometry and mapping with vehicle mounted cameras.},
author = {Scaramuzza, Davide and Fraundorfer, Friedrich and Pollefeys, Marc and Siegwart, Roland},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1413--1419},
title = {{Absolute scale in structure from motion from a single vehicle mounted camera by exploiting nonholonomic constraints}},
year = {2009}
}
@article{Shi1994,
abstract = {No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments},
author = {Shi, Jianbo Shi Jianbo and Tomasi, Carlo},
journal = {Computer Vision and Pattern Recognition, 1994. Proceedings CVPR '94., 1994 IEEE Computer Society Conference on},
number = {December},
pages = {593--600},
title = {{Good features to track}},
year = {1994}
}
@article{Song,
author = {Song, Shiyu and Guest, Clark C},
title = {{Parallel , Real-Time Monocular Visual Odometry}}
}
@article{Triggs2000,
abstract = {This paper is a survey of the theory and methods of photogrammetric bundle adjustment, aimed at potential implementors in the computer vision commu- nity. Bundle adjustment is the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates. Topics covered include: the choice of cost function and robustness; numerical optimization including sparse Newton methods, linearly convergent approximations, updating and recursive meth- ods; gauge (datum) invariance; and quality control. The theory is developed for general robust cost functions rather than restricting attention to traditional nonlinear least squares.},
author = {Triggs, Bill and Mclauchlan, Philip F. and Hartley, Richard I. and Fitzgibbon, Andrew W.},
journal = {Vision algorithms: theory and practice. S},
keywords = {Bundle Adjustment,Gauge Freedom,Optimization,Scene Reconstruction,Sparse Matrices,bundle adjustment,gauge freedom,optimization,scene reconstruction,sparse ma-,trices},
pages = {298--372},
title = {{Bundle Adjustment - a Modern Synthesis}},
volume = {34099},
year = {2000}
}
@article{fischer2015flownet,
  title={Flownet: Learning optical flow with convolutional networks},
  author={Fischer, Philipp and Dosovitskiy, Alexey and Ilg, Eddy and H{\"a}usser, Philip and Haz{\i}rba{\c{s}}, Caner and Golkov, Vladimir and van der Smagt, Patrick and Cremers, Daniel and Brox, Thomas},
  journal={arXiv preprint arXiv:1504.06852},
  year={2015}
}
@article{geiger2013vision,
  title={Vision meets robotics: The KITTI dataset},
  author={Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1231--1237},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@article{jia2014caffe,
  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal = {arXiv preprint arXiv:1408.5093},
  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  Year = {2014}
}
@article{DBLP:journals/corr/ZeilerF13,
  author    = {Matthew D. Zeiler and
               Rob Fergus},
  title     = {Visualizing and Understanding Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1311.2901},
  year      = {2013},
  url       = {http://arxiv.org/abs/1311.2901},
  timestamp = {Wed, 07 Jun 2017 14:40:45 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZeilerF13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{lowe1987three,
  title={Three-dimensional object recognition from single two-dimensional images},
  author={Lowe, David G},
  journal={Artificial intelligence},
  volume={31},
  number={3},
  pages={355--395},
  year={1987},
  publisher={Elsevier}
}
@inproceedings{scaramuzza2009absolute,
  title={Absolute scale in structure from motion from a single vehicle mounted camera by exploiting nonholonomic constraints},
  author={Scaramuzza, Davide and Fraundorfer, Friedrich and Pollefeys, Marc and Siegwart, Roland},
  booktitle={2009 IEEE 12th International Conference on Computer Vision},
  pages={1413--1419},
  year={2009},
  organization={IEEE}
}
@inproceedings{zhou2016reliable,
  title={Reliable scale estimation and correction for monocular Visual Odometry},
  author={Zhou, Dingfu and Dai, Yuchao and Li, Hongdong},
  booktitle={Intelligent Vehicles Symposium (IV), 2016 IEEE},
  pages={490--495},
  year={2016},
  organization={IEEE}
}
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@article{sermanet2013overfeat,
  title={Overfeat: Integrated recognition, localization and detection using convolutional networks},
  author={Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Micha{\"e}l and Fergus, Rob and LeCun, Yann},
  journal={arXiv preprint arXiv:1312.6229},
  year={2013}
}
@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}
@inproceedings{he2014spatial,
  title={Spatial pyramid pooling in deep convolutional networks for visual recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European Conference on Computer Vision},
  pages={346--361},
  year={2014},
  organization={Springer}
}
@article{ning2005toward,
  title={Toward automatic phenotyping of developing embryos from videos},
  author={Ning, Feng and Delhomme, Damien and LeCun, Yann and Piano, Fabio and Bottou, L{\'e}on and Barbano, Paolo Emilio},
  journal={IEEE Transactions on Image Processing},
  volume={14},
  number={9},
  pages={1360--1371},
  year={2005},
  publisher={IEEE}
}
@inproceedings{gupta2014learning,
  title={Learning rich features from RGB-D images for object detection and segmentation},
  author={Gupta, Saurabh and Girshick, Ross and Arbel{\'a}ez, Pablo and Malik, Jitendra},
  booktitle={European Conference on Computer Vision},
  pages={345--360},
  year={2014},
  organization={Springer}
}
@article{liu2016learning,
  title={Learning depth from single monocular images using deep convolutional neural fields},
  author={Liu, Fayao and Shen, Chunhua and Lin, Guosheng and Reid, Ian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={10},
  pages={2024--2039},
  year={2016},
  publisher={IEEE}
}
@article{fischer2015flownet,
  title={Flownet: Learning optical flow with convolutional networks},
  author={Fischer, Philipp and Dosovitskiy, Alexey and Ilg, Eddy and H{\"a}usser, Philip and Haz{\i}rba{\c{s}}, Caner and Golkov, Vladimir and van der Smagt, Patrick and Cremers, Daniel and Brox, Thomas},
  journal={arXiv preprint arXiv:1504.06852},
  year={2015}
}
@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3431--3440},
  year={2015}
}

